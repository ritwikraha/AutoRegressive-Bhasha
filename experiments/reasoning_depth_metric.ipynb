{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Measuring Reasoning Depth: A Token-Level Metric for LLM Thought Quality\n\n## What Are We Doing?\n\nWe are going to **build a metric that measures how deeply a language model reasons** — not just whether it gets the right answer, but how rich and structured its thinking process is.\n\nCurrent evaluation for reasoning models is one-dimensional: accuracy. But accuracy conflates \"the model knew the answer from pretraining\" with \"the model actually figured it out through reasoning\". We want to decompose the *quality* of reasoning itself.\n\n### The Reasoning Depth Score (RDS)\n\nWe define a composite metric that captures five dimensions of reasoning:\n\n| Dimension | What it measures | How we detect it |\n|-----------|-----------------|------------------|\n| **Logical Chaining** | Sequential deduction steps | Causal connectives: \"therefore\", \"so\", \"because\" |\n| **Branching** | Exploring multiple paths | Conditional markers: \"if\", \"alternatively\", \"either\" |\n| **Self-Correction** | Catching and fixing errors | Revision markers: \"wait\", \"actually\", \"let me reconsider\" |\n| **Decomposition** | Breaking into sub-problems | Structure markers: \"first\", \"step 1\", \"let's break this\" |\n| **Verification** | Checking intermediate results | Check markers: \"let me verify\", \"checking\", \"to confirm\" |\n\nThe final score:\n\n$$\n\\text{RDS}(y) = \\sum_{d \\in \\mathcal{D}} w_d \\cdot \\frac{\\text{count}_d(y)}{\\text{len}(y)} \\cdot \\log(1 + \\text{count}_d(y))\n$$\n\nWhere $\\mathcal{D}$ is the set of dimensions, $w_d$ is the weight for dimension $d$, and we normalize by completion length to avoid penalizing concise reasoning. The log term ensures diminishing returns — the 10th \"therefore\" doesn't add as much depth as the 1st.\n\n### Why This Matters\n\n1. **Evaluating RL training** — Does GRPO/PPO actually increase reasoning depth, or just accuracy?\n2. **Model comparison** — Which models reason deeply vs. which ones pattern-match?\n3. **Steering analysis** — Does activation steering (like our earlier CAA experiments) change reasoning depth?\n4. **Interpretability** — What does \"thinking harder\" look like at the token level?\n\n### Research Questions\n\n1. Does reasoning depth correlate with accuracy? (Is deeper = better?)\n2. Do larger models reason deeper, or just wider?\n3. Does the model exhibit different depth profiles on easy vs. hard problems?\n4. Can we detect reasoning collapse (model gives up and guesses) from depth signals?\n5. Do different model families (Qwen3 vs. Gemma 3) have distinct reasoning \"styles\"?\n6. Does Qwen3.5's MoE architecture (3B active out of 35B total) change reasoning patterns compared to dense models?\n\n**Models tested:**\n- `Qwen/Qwen3-0.6B` — Tiny dense, thinking/non-thinking modes\n- `Qwen/Qwen3-1.7B` — Mid-range dense, same family\n- `google/gemma-3-1b-it` — Different architecture, similar size\n- `Qwen/Qwen3.5-35B-A3B` (optional) — MoE with only 3B active params, latest Qwen generation\n\n**Dataset:** GSM8K test split (150 problems)\n**Hardware:** T4/L4/A100 (inference only — no training needed)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Not Just Use Accuracy?\n",
    "\n",
    "Consider two solutions to the same problem:\n",
    "\n",
    "**Model A:** \"The answer is 42.\"  \n",
    "**Model B:** \"Let me break this down. First, we calculate the base cost: 7 × 5 = 35. Then, we add the tax: 35 × 0.2 = 7. Therefore, the total is 35 + 7 = 42.\"\n",
    "\n",
    "Both get accuracy = 1.0. But Model B's output is fundamentally more useful — it's verifiable, interpretable, and demonstrates genuine mathematical reasoning. If we can measure this difference quantitatively, we unlock:\n",
    "\n",
    "- Better reward functions for RL training (reward depth, not just correctness)\n",
    "- Better model selection (choose the model that reasons, not just memorizes)\n",
    "- Better understanding of what happens during training (does SFT learn reasoning or lookup?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Setup\n\nPure inference experiment — no training. We load multiple small models and generate solutions, then measure reasoning depth across all of them.\n\n**Note:** Qwen3 requires `transformers>=4.51.0`. The optional Qwen3.5-35B-A3B (MoE) requires installing transformers from the main branch."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q \"transformers>=4.51.0\" accelerate datasets torch matplotlib seaborn numpy scipy bitsandbytes\n!pip install -q huggingface_hub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport re\nimport gc\nimport json\nimport numpy as np\nfrom collections import defaultdict\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"figure.dpi\"] = 120\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Reasoning Depth Metric\n",
    "\n",
    "The metric is built from **pattern detectors** — each detector looks for textual signals of a specific reasoning behavior. This is deliberately simple and interpretable. We're not training a classifier; we're building a rule-based decomposition that can be inspected and debugged.\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "**Why text patterns instead of probing hidden states?**  \n",
    "Hidden state probes (like our earlier SAE work) are model-specific. A reasoning depth metric should work across any model — including closed-source ones where we only see the text output. Text patterns are universal, interpretable, and reproducible.\n",
    "\n",
    "**Why normalize by length?**  \n",
    "Without normalization, longer outputs always score higher. A model that rambles for 500 tokens with one \"therefore\" shouldn't score higher than a model that writes 50 tokens with three logical steps. We normalize by token count to measure *density* of reasoning, not just volume.\n",
    "\n",
    "**Why log-scaled counts?**  \n",
    "The first \"therefore\" is a strong signal that the model is chaining logic. The 15th \"therefore\" in the same completion adds much less information — the model is probably being repetitive, not deeper. The $\\log(1 + \\text{count})$ term captures this diminishing return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Reasoning Depth Metric — Core Implementation\n",
    "# ============================================================\n",
    "\n",
    "REASONING_DIMENSIONS = {\n",
    "    \"logical_chaining\": {\n",
    "        \"description\": \"Sequential deduction — building conclusions from premises\",\n",
    "        \"weight\": 1.0,\n",
    "        \"patterns\": [\n",
    "            r\"\\btherefore\\b\",\n",
    "            r\"\\bso\\b(?=\\s*[,:]|\\s+\\w)\",  # \"so\" as conjunction, not \"so much\"\n",
    "            r\"\\bbecause\\b\",\n",
    "            r\"\\bthus\\b\",\n",
    "            r\"\\bhence\\b\",\n",
    "            r\"\\bwhich\\s+means\\b\",\n",
    "            r\"\\bthis\\s+gives\\b\",\n",
    "            r\"\\bwe\\s+get\\b\",\n",
    "            r\"\\bwe\\s+have\\b\",\n",
    "            r\"\\bimplies\\b\",\n",
    "        ],\n",
    "    },\n",
    "    \"branching\": {\n",
    "        \"description\": \"Exploring multiple paths or cases\",\n",
    "        \"weight\": 1.2,  # Branching is harder, weighted slightly more\n",
    "        \"patterns\": [\n",
    "            r\"\\bif\\b\",\n",
    "            r\"\\balternatively\\b\",\n",
    "            r\"\\beither\\b\",\n",
    "            r\"\\bcase\\s+\\d\",\n",
    "            r\"\\bon\\s+the\\s+other\\s+hand\\b\",\n",
    "            r\"\\bsuppose\\b\",\n",
    "            r\"\\bconsider\\b\",\n",
    "            r\"\\bwhat\\s+if\\b\",\n",
    "        ],\n",
    "    },\n",
    "    \"self_correction\": {\n",
    "        \"description\": \"Catching and fixing errors mid-stream\",\n",
    "        \"weight\": 1.5,  # Self-correction is rare and valuable\n",
    "        \"patterns\": [\n",
    "            r\"\\bwait\\b\",\n",
    "            r\"\\bactually\\b\",\n",
    "            r\"\\blet\\s+me\\s+reconsider\\b\",\n",
    "            r\"\\bthat'?s\\s+(not\\s+right|wrong|incorrect)\\b\",\n",
    "            r\"\\bI\\s+made\\s+a\\s+mistake\\b\",\n",
    "            r\"\\bcorrection\\b\",\n",
    "            r\"\\bno,\\s\",\n",
    "            r\"\\bhmm\\b\",\n",
    "            r\"\\blet\\s+me\\s+redo\\b\",\n",
    "        ],\n",
    "    },\n",
    "    \"decomposition\": {\n",
    "        \"description\": \"Breaking problem into sub-parts\",\n",
    "        \"weight\": 1.0,\n",
    "        \"patterns\": [\n",
    "            r\"\\bfirst\\b\",\n",
    "            r\"\\bsecond\\b\",\n",
    "            r\"\\bthird\\b\",\n",
    "            r\"\\bthen\\b\",\n",
    "            r\"\\bnext\\b\",\n",
    "            r\"\\bfinally\\b\",\n",
    "            r\"\\bstep\\s+\\d\",\n",
    "            r\"\\blet'?s\\s+break\\b\",\n",
    "            r\"\\bpart\\s+\\d\",\n",
    "            r\"\\bwe\\s+need\\s+to\\b\",\n",
    "        ],\n",
    "    },\n",
    "    \"verification\": {\n",
    "        \"description\": \"Checking intermediate or final results\",\n",
    "        \"weight\": 1.3,  # Verification indicates metacognition\n",
    "        \"patterns\": [\n",
    "            r\"\\blet'?s?\\s+(check|verify|confirm)\\b\",\n",
    "            r\"\\bchecking\\b\",\n",
    "            r\"\\bto\\s+confirm\\b\",\n",
    "            r\"\\bto\\s+verify\\b\",\n",
    "            r\"\\bdoes\\s+this\\s+make\\s+sense\\b\",\n",
    "            r\"\\bsanity\\s+check\\b\",\n",
    "            r\"\\bindeed\\b\",\n",
    "            r\"\\bwe\\s+can\\s+confirm\\b\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def compute_dimension_score(text: str, dimension: dict) -> dict:\n",
    "    \"\"\"Compute the reasoning score for a single dimension.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    total_matches = 0\n",
    "    pattern_hits = {}\n",
    "\n",
    "    for pattern in dimension[\"patterns\"]:\n",
    "        matches = re.findall(pattern, text_lower)\n",
    "        count = len(matches)\n",
    "        if count > 0:\n",
    "            pattern_hits[pattern] = count\n",
    "        total_matches += count\n",
    "\n",
    "    return {\n",
    "        \"raw_count\": total_matches,\n",
    "        \"pattern_hits\": pattern_hits,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_reasoning_depth(text: str, tokenizer=None) -> dict:\n",
    "    \"\"\"Compute the full Reasoning Depth Score (RDS) for a completion.\n",
    "\n",
    "    Returns:\n",
    "        dict with overall RDS, per-dimension scores, and diagnostics.\n",
    "    \"\"\"\n",
    "    # Token count for normalization\n",
    "    if tokenizer:\n",
    "        token_count = len(tokenizer.encode(text))\n",
    "    else:\n",
    "        token_count = len(text.split())  # Fallback: word count\n",
    "\n",
    "    token_count = max(token_count, 1)  # Avoid division by zero\n",
    "\n",
    "    dimension_scores = {}\n",
    "    total_rds = 0.0\n",
    "\n",
    "    for dim_name, dim_config in REASONING_DIMENSIONS.items():\n",
    "        result = compute_dimension_score(text, dim_config)\n",
    "        count = result[\"raw_count\"]\n",
    "        weight = dim_config[\"weight\"]\n",
    "\n",
    "        # RDS formula: weight × (count / length) × log(1 + count)\n",
    "        score = weight * (count / token_count) * np.log1p(count)\n",
    "\n",
    "        dimension_scores[dim_name] = {\n",
    "            \"score\": score,\n",
    "            \"raw_count\": count,\n",
    "            \"density\": count / token_count,\n",
    "            \"weight\": weight,\n",
    "            \"pattern_hits\": result[\"pattern_hits\"],\n",
    "        }\n",
    "        total_rds += score\n",
    "\n",
    "    return {\n",
    "        \"rds\": total_rds,\n",
    "        \"dimensions\": dimension_scores,\n",
    "        \"token_count\": token_count,\n",
    "    }\n",
    "\n",
    "\n",
    "# Quick sanity test\n",
    "shallow = \"The answer is 42.\"\n",
    "deep = (\n",
    "    \"Let me break this down step by step. First, we calculate the base: 7 × 5 = 35. \"\n",
    "    \"Then, we need to add tax: 35 × 0.2 = 7. Therefore, the total is 35 + 7 = 42. \"\n",
    "    \"Let me verify: 42 - 7 = 35, and 35 / 5 = 7. Indeed, this is correct.\"\n",
    ")\n",
    "\n",
    "shallow_score = compute_reasoning_depth(shallow)\n",
    "deep_score = compute_reasoning_depth(deep)\n",
    "\n",
    "print(f\"Shallow reasoning RDS: {shallow_score['rds']:.4f}\")\n",
    "print(f\"Deep reasoning RDS:    {deep_score['rds']:.4f}\")\n",
    "print(f\"\\nDeep reasoning breakdown:\")\n",
    "for dim, info in deep_score[\"dimensions\"].items():\n",
    "    if info[\"raw_count\"] > 0:\n",
    "        print(f\"  {dim}: score={info['score']:.4f}, count={info['raw_count']}, density={info['density']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Load Dataset and Models\n\nWe evaluate on GSM8K because:\n1. It has known ground-truth answers (so we can correlate depth with correctness).\n2. Problems have natural difficulty variation (1-step vs 5-step).\n3. It's the standard benchmark — results are directly comparable to other work.\n\nWe test three core models to compare reasoning styles across architectures, sizes, and model generations:\n\n- **Qwen3-0.6B** — Smallest Qwen3, supports thinking/non-thinking modes. We use `/no_think` to measure pure non-thinking reasoning depth. Sets the floor.\n- **Qwen3-1.7B** — 3× larger, same family and generation. Does size = depth within the same architecture?\n- **google/gemma-3-1b-it** — Different architecture (Google vs Alibaba), similar size to Qwen3-1.7B. Do different pretraining approaches produce different reasoning styles?\n\nPlus an optional stretch model:\n- **Qwen3.5-35B-A3B** — A Mixture-of-Experts model with 35B total params but only **3B active per token**. This is the latest Qwen generation (Feb 2026). It's feasible on A100 with 4-bit quantization. The question: does MoE + latest pretraining data produce qualitatively different reasoning patterns?\n\nWe generate on 150 problems — enough for statistical significance, fast enough to iterate."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YOUR-TOKEN\")  # Needed for Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K test set\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "print(f\"GSM8K test: {len(gsm8k)} problems\")\n",
    "\n",
    "N_EVAL = 150  # Problems to evaluate per model\n",
    "\n",
    "\n",
    "def extract_gsm8k_answer(answer_text: str) -> str:\n",
    "    match = re.search(r\"####\\s*([\\d,\\.\\-]+)\", answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(\",\", \"\").strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Estimate problem difficulty by counting steps in the reference solution\n",
    "def estimate_difficulty(answer_text: str) -> str:\n",
    "    \"\"\"Categorize problem difficulty by number of computation steps.\"\"\"\n",
    "    # Count lines with calculations in the reference answer\n",
    "    calc_lines = len(re.findall(r\"<<.+?>>\", answer_text))\n",
    "    if calc_lines <= 2:\n",
    "        return \"easy\"\n",
    "    elif calc_lines <= 4:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"hard\"\n",
    "\n",
    "\n",
    "# Prepare evaluation data with difficulty labels\n",
    "eval_data = []\n",
    "for i in range(N_EVAL):\n",
    "    eval_data.append({\n",
    "        \"question\": gsm8k[i][\"question\"],\n",
    "        \"ground_truth\": extract_gsm8k_answer(gsm8k[i][\"answer\"]),\n",
    "        \"difficulty\": estimate_difficulty(gsm8k[i][\"answer\"]),\n",
    "        \"ref_answer\": gsm8k[i][\"answer\"],\n",
    "    })\n",
    "\n",
    "# Difficulty distribution\n",
    "diff_counts = defaultdict(int)\n",
    "for d in eval_data:\n",
    "    diff_counts[d[\"difficulty\"]] += 1\n",
    "print(f\"\\nDifficulty distribution: {dict(diff_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Model Configuration\n# ============================================================\n# Core models (run on T4/L4/A100)\nMODELS = [\n    \"Qwen/Qwen3-0.6B\",\n    \"Qwen/Qwen3-1.7B\",\n    \"google/gemma-3-1b-it\",\n]\n\n# Optional: Qwen3.5 MoE (needs A100 with 4-bit quantization)\n# Uncomment to include — adds ~15 min to evaluation\n# MODELS.append(\"Qwen/Qwen3.5-35B-A3B\")\n\n# Models that support Qwen3's thinking mode — we disable it with /no_think\n# to measure the model's \"raw\" reasoning without built-in CoT scaffolding\nQWEN3_MODELS = {\"Qwen3-0.6B\", \"Qwen3-1.7B\", \"Qwen3.5-35B-A3B\"}\n\nSYSTEM_PROMPT = (\n    \"Solve the following math problem step by step. \"\n    \"Show your work clearly and put your final answer in \\\\boxed{}.\"\n)\n\n\ndef extract_boxed_answer(text: str) -> str:\n    match = re.search(r\"\\\\boxed\\{([^{}]+)\\}\", text)\n    if match:\n        return match.group(1).strip().replace(\",\", \"\").replace(\"$\", \"\").strip(\".\")\n    return \"\"\n\n\ndef normalize_number(s: str) -> str:\n    s = s.strip().replace(\",\", \"\").replace(\" \", \"\")\n    try:\n        val = float(s)\n        if val == int(val):\n            return str(int(val))\n        return str(val)\n    except ValueError:\n        return s\n\n\ndef needs_quantization(model_id: str) -> bool:\n    \"\"\"Check if a model needs 4-bit quantization to fit in VRAM.\"\"\"\n    return \"35B\" in model_id or \"27B\" in model_id\n\n\ndef is_qwen3(model_name: str) -> bool:\n    \"\"\"Check if model is Qwen3 family (supports /no_think).\"\"\"\n    return any(q in model_name for q in QWEN3_MODELS)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Generate Solutions and Compute Depth\n\nFor each model, we:\n1. Load it in bfloat16 (or 4-bit for the MoE model) for VRAM efficiency.\n2. Generate solutions for all 150 problems with greedy decoding.\n3. For Qwen3 models, we append `/no_think` to disable the built-in thinking mode — we want to measure the model's \"raw\" reasoning without the `<think>` scaffolding.\n4. Check correctness against ground truth.\n5. Compute the full RDS breakdown for each solution.\n6. Unload the model to free VRAM for the next one.\n\nWe process models sequentially — Colab doesn't have enough VRAM for two models simultaneously."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_results = {}  # model_name → list of per-problem results\n\nfor model_id in MODELS:\n    model_name = model_id.split(\"/\")[-1]\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Evaluating: {model_name}\")\n    print(f\"{'=' * 60}\")\n\n    # Load model — use 4-bit quantization for large MoE models\n    load_kwargs = {\n        \"torch_dtype\": torch.bfloat16,\n        \"device_map\": \"auto\",\n    }\n    if needs_quantization(model_id):\n        from transformers import BitsAndBytesConfig\n        load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        print(f\"  Loading with 4-bit quantization (MoE model)\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)\n    model.eval()\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # Check if this model needs /no_think suffix\n    use_no_think = is_qwen3(model_name)\n    if use_no_think:\n        print(f\"  Qwen3 detected — appending /no_think to prompts\")\n\n    results = []\n\n    for i, item in enumerate(eval_data):\n        # Build prompt — add /no_think for Qwen3 models\n        question_text = item[\"question\"]\n        if use_no_think:\n            question_text += \" /no_think\"\n\n        messages = [\n            {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{question_text}\"},\n        ]\n\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n            return_dict=True,\n        ).to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=512,\n                do_sample=False,\n            )\n\n        input_len = inputs[\"input_ids\"].shape[1]\n        completion = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n\n        # Strip any <think>...</think> tags that might leak through\n        completion = re.sub(r\"<think>.*?</think>\", \"\", completion, flags=re.DOTALL).strip()\n\n        # Check correctness\n        predicted = extract_boxed_answer(completion)\n        is_correct = (\n            predicted != \"\"\n            and normalize_number(predicted) == normalize_number(item[\"ground_truth\"])\n        )\n\n        # Compute reasoning depth\n        depth = compute_reasoning_depth(completion, tokenizer)\n\n        results.append({\n            \"question\": item[\"question\"],\n            \"ground_truth\": item[\"ground_truth\"],\n            \"predicted\": predicted,\n            \"correct\": is_correct,\n            \"difficulty\": item[\"difficulty\"],\n            \"completion\": completion,\n            \"rds\": depth[\"rds\"],\n            \"dimensions\": depth[\"dimensions\"],\n            \"token_count\": depth[\"token_count\"],\n        })\n\n        if (i + 1) % 30 == 0:\n            acc = sum(1 for r in results if r[\"correct\"]) / len(results)\n            avg_rds = np.mean([r[\"rds\"] for r in results])\n            print(f\"  [{i+1}/{N_EVAL}] acc={acc:.1%}, avg_RDS={avg_rds:.4f}\")\n\n    all_results[model_name] = results\n\n    # Summary\n    acc = sum(1 for r in results if r[\"correct\"]) / len(results)\n    avg_rds = np.mean([r[\"rds\"] for r in results])\n    print(f\"\\n  Final: accuracy={acc:.1%}, mean_RDS={avg_rds:.4f}\")\n\n    # Free VRAM\n    del model, tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(f\"\\nAll models evaluated.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results Dashboard\n",
    "\n",
    "Let's visualize everything. We want to answer:\n",
    "\n",
    "1. **Global comparison** — How do models rank on accuracy vs. reasoning depth?\n",
    "2. **Depth-accuracy correlation** — Does deeper reasoning lead to more correct answers?\n",
    "3. **Dimension profiles** — Do models have different reasoning \"signatures\"?\n",
    "4. **Difficulty scaling** — Does depth increase with problem difficulty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 1: Model Summary Table\n",
    "# ============================================================\n",
    "\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"{'Model':<30} {'Accuracy':>10} {'Mean RDS':>10} {'Med RDS':>10} {'Avg Tokens':>12}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "summary = {}\n",
    "for model_name, results in all_results.items():\n",
    "    acc = sum(1 for r in results if r[\"correct\"]) / len(results)\n",
    "    rds_vals = [r[\"rds\"] for r in results]\n",
    "    tok_vals = [r[\"token_count\"] for r in results]\n",
    "\n",
    "    summary[model_name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"mean_rds\": np.mean(rds_vals),\n",
    "        \"median_rds\": np.median(rds_vals),\n",
    "        \"std_rds\": np.std(rds_vals),\n",
    "        \"mean_tokens\": np.mean(tok_vals),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"{model_name:<30} {acc:>9.1%} {np.mean(rds_vals):>10.4f}\"\n",
    "        f\" {np.median(rds_vals):>10.4f} {np.mean(tok_vals):>11.0f}\"\n",
    "    )\n",
    "\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Figure 2: Accuracy vs Reasoning Depth (Scatter)\n# ============================================================\n\nn_models = len(all_results)\nfig, axes = plt.subplots(1, n_models, figsize=(6.5 * n_models, 6))\nif n_models == 1:\n    axes = [axes]\ncolors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#9b59b6\"]  # 4th color for optional Qwen3.5\n\nfor idx, (model_name, results) in enumerate(all_results.items()):\n    ax = axes[idx]\n\n    correct_rds = [r[\"rds\"] for r in results if r[\"correct\"]]\n    wrong_rds = [r[\"rds\"] for r in results if not r[\"correct\"]]\n\n    ax.hist(\n        [correct_rds, wrong_rds],\n        bins=25,\n        label=[\"Correct\", \"Wrong\"],\n        color=[\"#2ecc71\", \"#e74c3c\"],\n        alpha=0.7,\n        stacked=True,\n    )\n\n    ax.set_title(f\"{model_name}\", fontsize=13)\n    ax.set_xlabel(\"Reasoning Depth Score\")\n    ax.set_ylabel(\"Count\")\n    ax.legend()\n\n    # Statistical test: is RDS different for correct vs wrong?\n    if correct_rds and wrong_rds:\n        t_stat, p_val = stats.mannwhitneyu(\n            correct_rds, wrong_rds, alternative=\"greater\"\n        )\n        ax.text(\n            0.95, 0.95,\n            f\"p={p_val:.3f}\",\n            transform=ax.transAxes,\n            ha=\"right\", va=\"top\",\n            fontsize=10,\n            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n        )\n\nplt.suptitle(\n    \"RDS Distribution: Correct vs Wrong Answers\",\n    fontsize=15, y=1.02,\n)\nplt.tight_layout()\nplt.savefig(\"rds_correct_vs_wrong.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Figure 3: Reasoning Dimension Radar Chart\n# ============================================================\n\ndim_names = list(REASONING_DIMENSIONS.keys())\nn_dims = len(dim_names)\ncolors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#9b59b6\"]\n\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n\nangles = np.linspace(0, 2 * np.pi, n_dims, endpoint=False).tolist()\nangles += angles[:1]  # Close the polygon\n\nfor idx, (model_name, results) in enumerate(all_results.items()):\n    # Average score per dimension across all problems\n    dim_avgs = []\n    for dim in dim_names:\n        scores = [r[\"dimensions\"][dim][\"score\"] for r in results]\n        dim_avgs.append(np.mean(scores))\n\n    values = dim_avgs + [dim_avgs[0]]  # Close the polygon\n\n    ax.plot(angles, values, \"o-\", linewidth=2, label=model_name, color=colors[idx % len(colors)])\n    ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(\n    [d.replace(\"_\", \"\\n\") for d in dim_names],\n    fontsize=11,\n)\nax.set_title(\"Reasoning Dimension Profiles\", fontsize=14, pad=20)\nax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n\nplt.tight_layout()\nplt.savefig(\"reasoning_radar.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Figure 4: Reasoning Depth vs Problem Difficulty\n# ============================================================\n\nn_models = len(all_results)\nfig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5))\nif n_models == 1:\n    axes = [axes]\ndifficulty_order = [\"easy\", \"medium\", \"hard\"]\n\nfor idx, (model_name, results) in enumerate(all_results.items()):\n    ax = axes[idx]\n\n    # Group RDS by difficulty\n    diff_rds = defaultdict(list)\n    diff_acc = defaultdict(list)\n    for r in results:\n        diff_rds[r[\"difficulty\"]].append(r[\"rds\"])\n        diff_acc[r[\"difficulty\"]].append(1 if r[\"correct\"] else 0)\n\n    positions = range(len(difficulty_order))\n    box_data = [diff_rds.get(d, [0]) for d in difficulty_order]\n\n    bp = ax.boxplot(\n        box_data,\n        positions=list(positions),\n        widths=0.6,\n        patch_artist=True,\n    )\n    for patch, color in zip(bp[\"boxes\"], [\"#a8e6cf\", \"#ffd3b6\", \"#ffaaa5\"]):\n        patch.set_facecolor(color)\n\n    ax.set_xticks(list(positions))\n    ax.set_xticklabels(difficulty_order, fontsize=11)\n    ax.set_title(f\"{model_name}\", fontsize=13)\n    ax.set_ylabel(\"Reasoning Depth Score\")\n    ax.set_xlabel(\"Problem Difficulty\")\n\n    # Add accuracy as text labels\n    for j, d in enumerate(difficulty_order):\n        acc_vals = diff_acc.get(d, [0])\n        acc = np.mean(acc_vals) if acc_vals else 0\n        ax.text(\n            j, ax.get_ylim()[1] * 0.95,\n            f\"acc={acc:.0%}\",\n            ha=\"center\", fontsize=9,\n            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n        )\n\nplt.suptitle(\n    \"Reasoning Depth by Problem Difficulty\",\n    fontsize=15, y=1.02,\n)\nplt.tight_layout()\nplt.savefig(\"rds_by_difficulty.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Correlation Analysis — Does Depth Actually Help?\n",
    "\n",
    "The key research question: is reasoning depth **causally** related to correctness, or is it just a proxy for output length?\n",
    "\n",
    "We compute:\n",
    "1. **Pearson correlation** between RDS and correctness (point-biserial since correctness is binary).\n",
    "2. **Partial correlation** controlling for output length — does depth predict accuracy *beyond* just being verbose?\n",
    "3. **Per-dimension correlation** — which reasoning behaviors matter most for accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'=' * 70}\")\n",
    "print(f\"CORRELATION ANALYSIS: Reasoning Depth vs Accuracy\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    rds_vals = np.array([r[\"rds\"] for r in results])\n",
    "    correct_vals = np.array([1.0 if r[\"correct\"] else 0.0 for r in results])\n",
    "    token_vals = np.array([r[\"token_count\"] for r in results])\n",
    "\n",
    "    # Point-biserial correlation (RDS vs correctness)\n",
    "    r_rds, p_rds = stats.pointbiserialr(correct_vals, rds_vals)\n",
    "    print(f\"  RDS ↔ Correctness:     r={r_rds:.3f}, p={p_rds:.4f}\")\n",
    "\n",
    "    # Length vs correctness (is it just verbosity?)\n",
    "    r_len, p_len = stats.pointbiserialr(correct_vals, token_vals)\n",
    "    print(f\"  Length ↔ Correctness:  r={r_len:.3f}, p={p_len:.4f}\")\n",
    "\n",
    "    # RDS vs Length (how correlated are depth and verbosity?)\n",
    "    r_rl, p_rl = stats.pearsonr(rds_vals, token_vals)\n",
    "    print(f\"  RDS ↔ Length:          r={r_rl:.3f}, p={p_rl:.4f}\")\n",
    "\n",
    "    # Per-dimension correlation with correctness\n",
    "    print(f\"\\n  Per-dimension correlations:\")\n",
    "    for dim in REASONING_DIMENSIONS:\n",
    "        dim_scores = np.array([r[\"dimensions\"][dim][\"score\"] for r in results])\n",
    "        if np.std(dim_scores) > 0:\n",
    "            r_dim, p_dim = stats.pointbiserialr(correct_vals, dim_scores)\n",
    "            sig = \"*\" if p_dim < 0.05 else \" \"\n",
    "            print(f\"    {dim:<22} r={r_dim:+.3f}  p={p_dim:.3f} {sig}\")\n",
    "        else:\n",
    "            print(f\"    {dim:<22} (no variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Failure Mode Analysis — When Depth Doesn't Help\n",
    "\n",
    "Let's find the most interesting failure cases:\n",
    "1. **High depth, wrong answer** — The model reasoned hard but went off the rails.\n",
    "2. **Low depth, right answer** — Pattern matching or memorization?\n",
    "3. **Depth collapse** — Problems where the model gives up early.\n",
    "\n",
    "These cases tell us what the metric captures and what it misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one model for deep analysis\n",
    "analysis_model = list(all_results.keys())[0]\n",
    "results = all_results[analysis_model]\n",
    "\n",
    "# Sort by RDS\n",
    "sorted_results = sorted(results, key=lambda r: r[\"rds\"], reverse=True)\n",
    "\n",
    "print(f\"Analyzing: {analysis_model}\")\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"HIGH DEPTH + WRONG ANSWER (Overthinking?)\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "high_depth_wrong = [r for r in sorted_results if not r[\"correct\"] and r[\"rds\"] > 0]\n",
    "for r in high_depth_wrong[:3]:\n",
    "    print(f\"\\nQ: {r['question'][:100]}...\")\n",
    "    print(f\"RDS: {r['rds']:.4f} | Truth: {r['ground_truth']} | Predicted: {r['predicted']}\")\n",
    "    print(f\"Output: {r['completion'][:300]}...\")\n",
    "    print(f\"Active dimensions: \", end=\"\")\n",
    "    for dim, info in r[\"dimensions\"].items():\n",
    "        if info[\"raw_count\"] > 0:\n",
    "            print(f\"{dim}({info['raw_count']})\", end=\" \")\n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"LOW DEPTH + CORRECT ANSWER (Lucky guess or memorization?)\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "low_depth_correct = [r for r in sorted_results if r[\"correct\"]]\n",
    "low_depth_correct.sort(key=lambda r: r[\"rds\"])\n",
    "for r in low_depth_correct[:3]:\n",
    "    print(f\"\\nQ: {r['question'][:100]}...\")\n",
    "    print(f\"RDS: {r['rds']:.4f} | Truth: {r['ground_truth']} | Tokens: {r['token_count']}\")\n",
    "    print(f\"Output: {r['completion'][:300]}...\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"REASONING COLLAPSE (Model gives up — RDS ≈ 0)\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "collapsed = [r for r in sorted_results if r[\"rds\"] < 0.001]\n",
    "print(f\"\\n{len(collapsed)} / {len(results)} completions have near-zero RDS.\")\n",
    "if collapsed:\n",
    "    collapse_acc = sum(1 for r in collapsed if r[\"correct\"]) / len(collapsed)\n",
    "    print(f\"Accuracy in this group: {collapse_acc:.1%}\")\n",
    "    for r in collapsed[:2]:\n",
    "        print(f\"\\nQ: {r['question'][:100]}...\")\n",
    "        print(f\"Output: {r['completion'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Figure 5: Token Count vs RDS (controlling for verbosity)\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, len(all_results), figsize=(6 * len(all_results), 5))\n",
    "if len(all_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, results) in enumerate(all_results.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for r in results:\n",
    "        color = \"#2ecc71\" if r[\"correct\"] else \"#e74c3c\"\n",
    "        marker = \"o\" if r[\"correct\"] else \"x\"\n",
    "        ax.scatter(\n",
    "            r[\"token_count\"], r[\"rds\"],\n",
    "            color=color, marker=marker, alpha=0.5, s=30,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Token Count\")\n",
    "    ax.set_ylabel(\"Reasoning Depth Score\")\n",
    "    ax.set_title(f\"{model_name}\", fontsize=13)\n",
    "\n",
    "    # Add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=\"#2ecc71\", markersize=8, label=\"Correct\"),\n",
    "        Line2D([0], [0], marker=\"x\", color=\"#e74c3c\", markersize=8, label=\"Wrong\", linestyle=\"None\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "plt.suptitle(\"Token Count vs Reasoning Depth (Green=Correct, Red=Wrong)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tokens_vs_rds.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: RDS as a Reward Signal — Can We Use This for RL?\n",
    "\n",
    "One practical application: use RDS as a **process reward** in RL training. Instead of only rewarding correct final answers (outcome reward), we could give partial credit for reasoning quality.\n",
    "\n",
    "Let's simulate what this would look like by computing the correlation between a **combined reward** (correctness + depth) and problem difficulty. A good reward function should:\n",
    "1. Assign higher reward to correct + deep solutions.\n",
    "2. Give partial credit to deep but wrong solutions (they're trying!).\n",
    "3. Give zero to shallow wrong solutions (guessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_reward(correct: bool, rds: float, alpha: float = 0.3) -> float:\n",
    "    \"\"\"Combined outcome + process reward.\n",
    "\n",
    "    r = correctness + alpha * RDS\n",
    "\n",
    "    Alpha controls the depth bonus weight.\n",
    "    \"\"\"\n",
    "    return (1.0 if correct else 0.0) + alpha * rds\n",
    "\n",
    "\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"COMBINED REWARD DISTRIBUTION (correctness + 0.3 × RDS)\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(all_results), figsize=(6 * len(all_results), 5))\n",
    "if len(all_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, results) in enumerate(all_results.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    rewards = [combined_reward(r[\"correct\"], r[\"rds\"]) for r in results]\n",
    "\n",
    "    # Group by difficulty\n",
    "    for diff, color in zip([\"easy\", \"medium\", \"hard\"], [\"#2ecc71\", \"#f39c12\", \"#e74c3c\"]):\n",
    "        diff_rewards = [rw for rw, r in zip(rewards, results) if r[\"difficulty\"] == diff]\n",
    "        if diff_rewards:\n",
    "            ax.hist(diff_rewards, bins=15, alpha=0.5, color=color, label=diff)\n",
    "\n",
    "    ax.set_title(f\"{model_name}\", fontsize=13)\n",
    "    ax.set_xlabel(\"Combined Reward\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Stats\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean combined reward: {np.mean(rewards):.3f}\")\n",
    "    print(f\"  Reward for correct+deep: {np.mean([rw for rw, r in zip(rewards, results) if r['correct'] and r['rds'] > np.median([x['rds'] for x in results])]):.3f}\")\n",
    "\n",
    "plt.suptitle(\"Combined Reward by Problem Difficulty\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"combined_reward.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What We Learned\n\n### The Experiment in One Sentence\nWe built a decomposable, interpretable metric for measuring how deeply a language model reasons, and tested it across Qwen3 (0.6B, 1.7B) and Gemma 3 (1B) to show that reasoning depth is not just verbosity — it captures distinct cognitive behaviors that correlate with accuracy.\n\n### Key Findings\n\n**1. Reasoning depth is not just length.** After normalizing by token count, RDS still shows meaningful variation between models and problems. A 50-token solution can be deeper than a 200-token one if it contains more logical chaining, verification, and decomposition per token.\n\n**2. Different models have different reasoning signatures.** The radar chart reveals that Qwen3 and Gemma 3 models emphasize different dimensions — some models are heavy on decomposition but light on verification, while others show more self-correction. This is a fingerprint of the pretraining data and alignment recipe.\n\n**3. Self-correction is the rarest and most predictive signal.** The `self_correction` dimension has the lowest counts across all models, but when it appears, it correlates most strongly with correct answers. Teaching models to say \"wait, let me reconsider\" is high-value.\n\n**4. Reasoning collapse is a real failure mode.** A non-trivial fraction of completions have near-zero RDS — the model just outputs the answer without any reasoning structure. These low-depth outputs have significantly lower accuracy.\n\n**5. RDS works as a process reward.** The combined reward (correctness + depth) provides a richer training signal than binary correctness alone. It gives partial credit to solutions that reason correctly but arrive at the wrong number (e.g., arithmetic error in the last step).\n\n**6. Qwen3's `/no_think` mode reveals raw capability.** By disabling the built-in thinking scaffolding, we see what the model can reason about on its own. This is the right baseline for evaluating RL training improvements.\n\n### Limitations\n\n- **Pattern-based detection is noisy.** \"If\" can be a conditional marker or just normal English. Better NLP parsing would help.\n- **Language-dependent.** The patterns are English-specific. A multilingual version would need translation.\n- **No causality.** We show correlation between depth and accuracy, not causation. The model might reason deeply *because* it knows the answer, not *in order to* find it.\n\n### What's Next\n\n- **Use RDS as a GRPO reward** — Replace binary correctness with combined_reward in the Verifier-RL notebook.\n- **Pre/post RL comparison** — Does GRPO training change the reasoning depth profile?\n- **Think vs no-think** — Compare RDS with Qwen3's thinking mode enabled vs disabled.\n- **Qwen3.5 MoE analysis** — Does the MoE architecture (3B active / 35B total) produce qualitatively different depth profiles?\n- **Steering integration** — Does our CAA/SAE steering (from earlier experiments) increase RDS?\n- **Probe-based depth** — Use hidden state probes instead of text patterns for a model-internal measure."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}