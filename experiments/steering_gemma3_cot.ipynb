{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Gemma-3 Toward Chain-of-Thought Reasoning\n",
    "\n",
    "## What Are We Doing?\n",
    "\n",
    "We are going to **hack the residual stream** of a language model at inference time to force it to \"think more\" about math problems.\n",
    "\n",
    "The technique is called **Contrastive Activation Addition (CAA)**. The idea is simple:\n",
    "\n",
    "1. Show the model examples of **detailed, step-by-step reasoning** (positive) and **blunt, direct answers** (negative).\n",
    "2. Record the internal activations (hidden states) for both.\n",
    "3. Subtract negative from positive to get a **\"reasoning direction\"** vector.\n",
    "4. During generation, **inject** that vector into the residual stream so the model is nudged toward verbose reasoning.\n",
    "\n",
    "This is essentially an **inference-time alternative to RLHF** — instead of updating weights to maximize a reasoning reward, we directly perturb the latent trajectory to enforce reasoning behaviors. No fine-tuning, no few-shot prompting, just raw tensor surgery.\n",
    "\n",
    "**Target model:** `google/gemma-3-1b-it` (26 layers, 1152 hidden dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Model Loading\n",
    "\n",
    "We load Gemma-3-1B-IT with automatic device detection. On Colab with a GPU we use `bfloat16` for VRAM efficiency. On Apple Silicon Macs (MPS) we use `float32` since MPS has limited bfloat16 support. The model has **26 transformer layers** — we'll be hooking into the middle layers where high-level task representations tend to crystallize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"YOUR-TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "HF_TOKEN = \"YOUR-TOKEN\"\n",
    "\n",
    "# Device selection: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    model_dtype = torch.bfloat16\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    model_dtype = torch.float32  # MPS has limited bfloat16 support\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "print(f\"Using device: {device}, dtype: {model_dtype}\")\n",
    "print(\"Loading model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=model_dtype,\n",
    "    token=HF_TOKEN,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    ")\n",
    "\n",
    "# For MPS/CPU, manually move the model to device\n",
    "if device != \"cuda\":\n",
    "    model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded. Layers: {len(model.model.layers)}, Hidden dim: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Contrastive Dataset\n",
    "\n",
    "This is the heart of CAA. We build paired examples:\n",
    "\n",
    "- **Positive (CoT):** The model sees a question answered with full step-by-step breakdown.\n",
    "- **Negative (Direct):** The same question, but answered with just the final number.\n",
    "\n",
    "When we subtract the negative activations from the positive ones, we isolate the **direction in activation space** that corresponds to \"reasoning verbosely\" vs \"answering bluntly\". This difference vector *is* the concept of Chain-of-Thought, encoded as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\n",
    "        \"Question: What is 15 * 4? Answer: Let's break it down. 10 * 4 = 40. 5 * 4 = 20. 40 + 20 = 60. The answer is 60.\",\n",
    "        \"Question: What is 15 * 4? Answer: 60.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Question: If I have 3 apples and buy 5 more, then give away 2, how many do I have? Answer: Start with 3. Buy 5, so 3 + 5 = 8. Give away 2, so 8 - 2 = 6. The answer is 6.\",\n",
    "        \"Question: If I have 3 apples and buy 5 more, then give away 2, how many do I have? Answer: 6.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Question: Solve 2x + 5 = 15. Answer: First, subtract 5 from both sides to get 2x = 10. Then, divide by 2 to get x = 5. The answer is x=5.\",\n",
    "        \"Question: Solve 2x + 5 = 15. Answer: x=5.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Question: What is 144 / 12? Answer: I know that 12 * 12 = 144, so 144 divided by 12 is 12. The answer is 12.\",\n",
    "        \"Question: What is 144 / 12? Answer: 12.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Question: A train travels 60 km/h for 2.5 hours. How far does it go? Answer: Distance = speed * time. So distance = 60 * 2.5. 60 * 2 = 120 and 60 * 0.5 = 30. 120 + 30 = 150. The answer is 150 km.\",\n",
    "        \"Question: A train travels 60 km/h for 2.5 hours. How far does it go? Answer: 150 km.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Format each prompt through the chat template so the model sees them\n",
    "# in its expected conversational format\n",
    "pos_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": p[0]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    ) for p in pairs\n",
    "]\n",
    "neg_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": p[1]}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    ) for p in pairs\n",
    "]\n",
    "\n",
    "print(f\"Created {len(pairs)} contrastive pairs.\")\n",
    "print(f\"\\nExample positive prompt (raw):\\n{pos_prompts[0][:200]}...\")\n",
    "print(f\"\\nExample negative prompt (raw):\\n{neg_prompts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Vector Extractor (Representation Reading)\n",
    "\n",
    "This is where the actual science happens. Here's the algorithm:\n",
    "\n",
    "1. Attach a **`register_forward_hook`** to a specific transformer layer.\n",
    "2. Run each positive and negative prompt through the model (forward pass only, no generation).\n",
    "3. The hook intercepts the hidden states and grabs the **last token's activation** — this is the token that has accumulated the most context about the full prompt.\n",
    "4. Average the positive activations, average the negative activations.\n",
    "5. Subtract: `v_reason = mean(pos) - mean(neg)`\n",
    "6. Normalize the resulting vector to unit length (prevents activation explosions when we inject it later).\n",
    "\n",
    "The result is a single vector in $\\mathbb{R}^{1152}$ that points in the direction of \"step-by-step reasoning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steering_vector(model, tokenizer, pos_prompts, neg_prompts, layer_idx):\n",
    "    \"\"\"Extract a steering vector via contrastive activation differencing.\"\"\"\n",
    "    pos_acts = []\n",
    "    neg_acts = []\n",
    "    cache = []\n",
    "\n",
    "    def cache_hook(module, input, output):\n",
    "        # output is a tuple; output[0] is hidden states: [batch, seq_len, hidden_dim]\n",
    "        # Grab the last token's activation and move to CPU to save VRAM\n",
    "        hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "        cache.append(hidden_states[:, -1, :].detach().cpu())\n",
    "\n",
    "    # Hook into the target decoder layer\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(cache_hook)\n",
    "\n",
    "    print(f\"Extracting activations from layer {layer_idx}...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (pos, neg) in enumerate(zip(pos_prompts, neg_prompts)):\n",
    "            # Process positive example\n",
    "            pos_inputs = tokenizer(pos, return_tensors=\"pt\").to(device)\n",
    "            model(**pos_inputs)\n",
    "            pos_acts.append(cache.pop())\n",
    "\n",
    "            # Process negative example\n",
    "            neg_inputs = tokenizer(neg, return_tensors=\"pt\").to(device)\n",
    "            model(**neg_inputs)\n",
    "            neg_acts.append(cache.pop())\n",
    "\n",
    "            print(f\"  Pair {i+1}/{len(pos_prompts)} done.\")\n",
    "\n",
    "    # Remove the hook — critical so it doesn't fire during generation later\n",
    "    handle.remove()\n",
    "\n",
    "    # Stack into tensors and compute mean activations\n",
    "    pos_tensor = torch.stack(pos_acts).mean(dim=0)  # [1, hidden_dim]\n",
    "    neg_tensor = torch.stack(neg_acts).mean(dim=0)  # [1, hidden_dim]\n",
    "\n",
    "    # The steering vector: direction from \"blunt\" toward \"reasoning\"\n",
    "    steering_vec = pos_tensor - neg_tensor\n",
    "\n",
    "    # Normalize to unit length to prevent activation magnitude issues\n",
    "    steering_vec = steering_vec / torch.norm(steering_vec)\n",
    "\n",
    "    print(f\"Steering vector extracted. Shape: {steering_vec.shape}, Norm: {torch.norm(steering_vec).item():.4f}\")\n",
    "    return steering_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Vector\n",
    "\n",
    "Gemma-3-1B has **26 layers** (indices 0–25). We target **layer 13** — right in the middle. This is where high-level semantic representations (like \"am I reasoning step-by-step?\") tend to live, before the upper layers start routing toward specific vocabulary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LAYER = 13\n",
    "reasoning_vector = get_steering_vector(model, tokenizer, pos_prompts, neg_prompts, TARGET_LAYER)\n",
    "\n",
    "# Free up memory from the extraction phase\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device == \"mps\":\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Injection Controller (Representation Control)\n",
    "\n",
    "Now we use the extracted vector during generation. The injection hook does one thing:\n",
    "\n",
    "$$\\tilde{h}_l = h_l + \\alpha \\cdot \\vec{v}_{\\text{reason}}$$\n",
    "\n",
    "Where:\n",
    "- $h_l$ = the hidden state at layer $l$ for the token currently being generated\n",
    "- $\\vec{v}_{\\text{reason}}$ = our normalized steering vector\n",
    "- $\\alpha$ = a scalar multiplier controlling how hard we push\n",
    "\n",
    "We only modify the **last token** (`[:, -1, :]`) — that's the one actively being generated in the autoregressive loop. Previous tokens in the KV cache are untouched.\n",
    "\n",
    "**Alpha tuning guide:**\n",
    "- `0`: No effect (baseline)\n",
    "- `5–15`: Subtle nudge toward more reasoning\n",
    "- `15–30`: Strong reasoning push\n",
    "- `50+`: Likely gibberish — you've pushed the activations so far off the manifold that the model can't recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(model, tokenizer, prompt, steering_vector, layer_idx, alpha, max_new_tokens=256):\n",
    "    \"\"\"Generate text with a steering vector injected into the residual stream.\"\"\"\n",
    "    # Tokenize through the chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Move steering vector to GPU in the model's dtype\n",
    "    vec = steering_vector.to(device, dtype=model.dtype)\n",
    "\n",
    "    def injection_hook(module, input, output):\n",
    "        hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "        # Add the scaled reasoning vector to the current generation token\n",
    "        hidden_states[:, -1, :] += alpha * vec\n",
    "        # Repackage the output tuple so downstream layers see the modified states\n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_states,) + output[1:]\n",
    "        return hidden_states\n",
    "\n",
    "    # Register the injection hook\n",
    "    handle = model.model.layers[layer_idx].register_forward_hook(injection_hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    # CRITICAL: always remove the hook after generation\n",
    "    handle.remove()\n",
    "\n",
    "    # Decode only the newly generated tokens (skip the prompt)\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    return tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: The Experiment\n",
    "\n",
    "Let's put it all together. We'll throw a genuinely hard multi-step problem at the model — one that requires setting up equations, substitution, and careful arithmetic. We compare:\n",
    "\n",
    "1. **Baseline** (`alpha=0`): The model generates normally, no intervention.\n",
    "2. **Steered** (`alpha=15`): The reasoning vector is injected at every generation step.\n",
    "\n",
    "If CAA works, the steered output should be noticeably more verbose and step-by-step compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"A factory produces widgets in two shifts. The day shift produces 3 times as many widgets as the night shift. If the night shift produces x widgets, and 15% of all widgets from both shifts are defective, and each non-defective widget sells for $12, how much revenue does the factory make if the night shift produces 120 widgets?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE (alpha=0, no steering)\")\n",
    "print(\"=\" * 60)\n",
    "baseline = generate_with_steering(\n",
    "    model, tokenizer, test_prompt, reasoning_vector, TARGET_LAYER, alpha=0.0\n",
    ")\n",
    "print(baseline)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEERED (alpha=15, reasoning vector injected)\")\n",
    "print(\"=\" * 60)\n",
    "steered = generate_with_steering(\n",
    "    model, tokenizer, test_prompt, reasoning_vector, TARGET_LAYER, alpha=15.0\n",
    ")\n",
    "print(steered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sweep Across Alpha Values\n",
    "\n",
    "Let's see how the model's behavior changes as we crank up the steering strength. This gives us intuition about the \"reasoning manifold\" — at what point does the model go from normal to verbose to incoherent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_prompt = \"A train leaves City A at 9:00 AM traveling at 80 km/h toward City B. Another train leaves City B at 10:00 AM traveling at 120 km/h toward City A. If the cities are 560 km apart, at what time do the trains meet?\"\n",
    "\n",
    "for alpha in [0, 5, 10, 20, 35]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Alpha = {alpha}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    result = generate_with_steering(\n",
    "        model, tokenizer, sweep_prompt, reasoning_vector, TARGET_LAYER,\n",
    "        alpha=float(alpha), max_new_tokens=300\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "We built a **steering mechanism from scratch** using only PyTorch hooks:\n",
    "\n",
    "1. **Extracted** a reasoning direction from contrastive examples\n",
    "2. **Injected** it into the residual stream during autoregressive generation\n",
    "3. **Observed** how increasing $\\alpha$ pushes the model from terse answers toward verbose step-by-step reasoning (and eventually into gibberish)\n",
    "\n",
    "This is the core idea behind **representation engineering** — you don't need to retrain the model, you just need to know which direction to push in activation space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
